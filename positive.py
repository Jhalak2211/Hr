# -*- coding: utf-8 -*-
"""Positive.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z1Yxs6JnsZA7SIV8azMpuDjKfKaWo2BP
"""

# pip install torch torchvision torchaudio transformers datasets shap lime scikit-learn matplotlib --quiet

from datasets import load_dataset
import os

# Extend timeout (default is 10 seconds)
os.environ["HF_HUB_HTTP_TIMEOUT"] = "60"

# Try again
dataset = load_dataset("imdb")

import tensorflow_datasets as tfds
import pandas as pd

data, info = tfds.load("imdb_reviews", with_info=True, as_supervised=True)
train_data, test_data = data["train"], data["test"]

# Convert to pandas for consistency
train_texts, train_labels = [], []
for text, label in tfds.as_numpy(train_data):
    train_texts.append(text.decode("utf-8"))
    train_labels.append(int(label))

test_texts, test_labels = [], []
for text, label in tfds.as_numpy(test_data):
    test_texts.append(text.decode("utf-8"))
    test_labels.append(int(label))

train_df = pd.DataFrame({"text": train_texts, "label": train_labels})
test_df = pd.DataFrame({"text": test_texts, "label": test_labels})

print("‚úÖ Loaded IMDB dataset (TensorFlow version)")
print(train_df.head())

# ================================================
# üí¨ Sentiment Classification + Explainability
# Model: BERT-base-uncased | Dataset: IMDB (TFDS)
# ================================================

# !pip install -U transformers datasets tensorflow_datasets shap lime scikit-learn torch matplotlib --quiet

# -------------------------------
# STEP 1Ô∏è‚É£: Load IMDB dataset
# -------------------------------
import tensorflow_datasets as tfds
import pandas as pd

data, info = tfds.load("imdb_reviews", with_info=True, as_supervised=True)
train_data, test_data = data["train"], data["test"]

# Convert to pandas DataFrame
train_texts, train_labels = [], []
for text, label in tfds.as_numpy(train_data):
    train_texts.append(text.decode("utf-8"))
    train_labels.append(int(label))

test_texts, test_labels = [], []
for text, label in tfds.as_numpy(test_data):
    test_texts.append(text.decode("utf-8"))
    test_labels.append(int(label))

train_df = pd.DataFrame({"text": train_texts, "label": train_labels})
test_df = pd.DataFrame({"text": test_texts, "label": test_labels})

print("‚úÖ Loaded IMDB dataset (TensorFlow version)")
print(train_df.head())

# -------------------------------
# STEP 2Ô∏è‚É£: Tokenize + Prepare
# -------------------------------
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
from sklearn.metrics import accuracy_score, f1_score
import numpy as np

train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

train_dataset = train_dataset.rename_column("label", "labels")
test_dataset = test_dataset.rename_column("label", "labels")

train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

# -------------------------------
# STEP 3Ô∏è‚É£: Load Model + Train
# -------------------------------
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

def compute_metrics(pred):
    preds = np.argmax(pred.predictions, axis=1)
    labels = pred.label_ids
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds)
    }

training_args = TrainingArguments(
    output_dir="./bert_sentiment_model",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_dir="./logs",
    do_train=True,
    do_eval=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset.select(range(5000)),  # subset for faster demo
    eval_dataset=test_dataset.select(range(1000)),
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

print("\nüöÄ Training model...")
trainer.train()

print("\nüìä Evaluating model...")
results = trainer.evaluate()
print("‚úÖ Evaluation Results:", results)

model.save_pretrained("./bert_sentiment_model")
tokenizer.save_pretrained("./bert_sentiment_model")

# -------------------------------
# STEP 4Ô∏è‚É£: Inference Pipeline
# -------------------------------
from transformers import pipeline

classifier = pipeline("text-classification", model="./bert_sentiment_model", tokenizer="./bert_sentiment_model")

examples = [
    "I absolutely loved this movie! It was beautiful.",
    "This was the worst experience ever. I hated it."
]

for text in examples:
    print(f"{text} ‚Üí {classifier(text)}")

# -------------------------------
# STEP 5Ô∏è‚É£: Explainability (SHAP)
# -------------------------------
import shap

print("\nüß† Generating SHAP explanations...")

explainer = shap.Explainer(classifier, masker=shap.maskers.Text(tokenizer))

sample_texts = [
    "The movie was touching and full of emotion.",
    "This film was boring and poorly acted."
]

shap_values = explainer(sample_texts)

# Visualize token contributions
shap.plots.text(shap_values[0])
shap.plots.text(shap_values[1])

# -------------------------------
# STEP 6Ô∏è‚É£: Explainability (LIME)
# -------------------------------
from lime.lime_text import LimeTextExplainer
import numpy as np

print("\nüß† Generating LIME explanation...")

def predict_proba(texts):
    results = classifier(texts)
    probs = np.array([[r[0]['score'], r[1]['score']] for r in results])
    return probs

explainer = LimeTextExplainer(class_names=["Negative", "Positive"])

text = "The storyline was very interesting and engaging."
exp = explainer.explain_instance(text, predict_proba, num_features=8)
exp.show_in_notebook(text=True)

print("‚úÖ All steps completed successfully!")